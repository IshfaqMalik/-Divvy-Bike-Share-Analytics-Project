# ğŸš² Divvy Bike Share Analytics Project
**Azure Synapse Analytics + PostgreSQL Integration**

---

## ğŸ“Œ Project Summary

This project builds a cloud-based data warehouse in **Azure Synapse Analytics** using public Divvy Bike Share data from Chicago. The goal is to analyze ride behavior and payment trends to support key business outcomes.

---

## ğŸ“‚ Tasks Overview

### ğŸ›  Task 1: Azure Resources Setup

âœ… Steps:
- [x] **Azure Database for PostgreSQL** â€“ OLTP source system  
- [x] **Azure Synapse Workspace** â€“ Data warehouse  
- [x] **Serverless SQL Pool** â€“ Used for querying & transformation  
- â—ï¸Note: No dedicated SQL pool used (only serverless pool supported in lab)

ğŸ“¸ _Screenshot_: *PostgreSQL and Synapse resource setup*  
ğŸ–¼ï¸ `Azureresources.png`

---

### â­ Task 2: Star Schema Design

You designed a **star schema** based on:
- Business goals (duration, spending, rider behavior)
- Source relational schema from PostgreSQL

ğŸ“ Schema includes:
- `fact_rides`  
- `fact_payment`  
- `dim_rider`  
- `dim_station`  
- `dim_date` (excluded in your case)  
- `dim_time` (optional)  

ğŸ“¸ _Screenshot_: *ERD / Star Schema*  
ğŸ–¼ï¸ `images/task2_star_schema.png`

---

### ğŸ“ƒ Task 3: Create Data in PostgreSQL

âœ… Used Python script: `ProjectDataToPostgres.py`  
- Populates 4 tables in PostgreSQL with CSV data  
- Verified using pgAdmin  

ğŸ“¸ _Screenshot_: *PostgreSQL table preview*  
ğŸ–¼ï¸ `images/task3_postgres_tables.png`

---

### ğŸ“„ Task 4: Extract Data to Azure Blob Storage

âœ… Used **Synapse Ingest Wizard** to extract data from PostgreSQL to Azure Blob Storage.  
- Data for all 4 tables exported as `.csv` files  
- Files now accessible via Data Lake Linked Service

ğŸ“¸ _Screenshot_: *Ingest pipeline overview*  
ğŸ–¼ï¸ `images/task4_extract_pipeline.png`

---

### ğŸ“… Task 5: Load Data into External Tables

âœ… Used **auto-generated scripts** to create **external staging tables** in serverless SQL pool.  
- Loaded CSVs into Synapse external tables using `CREATE EXTERNAL TABLE`

ğŸ“¸ _Screenshot_: *External table setup in Synapse*  
ğŸ–¼ï¸ `images/task5_external_tables.png`

---

### ğŸ”„ Task 6: Transform to Star Schema (CETAS)

âœ… Used **CETAS (Create External Table AS SELECT)** to:
- Join & transform staging data
- Materialize star schema tables (`fact_rides`, `fact_payment`, etc.)

ğŸ§¹ Highlights:
- Defined file format `SynapseDelimitedTextFormat`
- Reused external data source
- Created output folders like `/fact_payment/`, `/fact_rides/`

ğŸ“¸ _Screenshot_: *CETAS transformation in Synapse Studio*  
ğŸ–¼ï¸ `images/task6_cetas_fact_payment.png`

---

## ğŸ” Sample Analytics Questions

ğŸ’¬ **Ride Analysis**
- Average duration by station and hour
- Member vs. casual rider behavior
- Rider age impact on ride length

ğŸ’¬ **Payment Analysis**
- Total revenue by quarter
- Member spending by age
- Cost per ride / per rider / per month

---

## âœ… Deliverables

- â˜‘ï¸ Star schema with `fact_rides` and `fact_payment`
- â˜‘ï¸ Working serverless SQL queries over external tables
- â˜‘ï¸ Screenshots showing setup, data flow, and CETAS outputs
- â˜‘ï¸ Blob Storage linked with Synapse Workspace

---

## ğŸ“Œ Notes

- No `dim_date` table used: all temporal features derived from timestamps
- External tables are **read-only views** into blob storage
- Star schema transformations are persisted as **partitioned CSVs** in Data Lake

